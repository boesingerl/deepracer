{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48535011",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "This notebook contains a selfmade REINFORCE algorithm implementation from Artificial Neural networks course. \n",
    "Wanted to use it as a baseline but it's not really adapted to the situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd3fd94e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'agents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b20465158107>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepracer_base_agent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDeepracerAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'agents'"
     ]
    }
   ],
   "source": [
    "class BasicAgent(DeepracerAgent):\n",
    "    def __init__(self, gamma=0.99, policy_learning_rate=0.002, value_learning_rate=0.002):\n",
    "\n",
    "        self.plr = policy_learning_rate\n",
    "        self.vlr = value_learning_rate\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.actions_prob = []\n",
    "        self.saved_rewards = []\n",
    "        self.model = None\n",
    "        self.value_model = None\n",
    "        self.n_moving_avg = 5\n",
    "\n",
    "        # These lists stores the cumulative observations for this episode\n",
    "        self.episode_observations, self.episode_actions, self.episode_rewards = [], [], []\n",
    "\n",
    "        # Build the keras network\n",
    "        self._build_network()\n",
    "\n",
    "    def _convert_obs(self, observation):\n",
    "        return segment_resize(observation['STEREO_CAMERAS'][:, :, 0])\n",
    "\n",
    "    def observe(self, state, action, reward):\n",
    "        \"\"\" This function takes the observations the agent received from the environment and stores them\n",
    "            in the lists above.\"\"\"\n",
    "        self.episode_observations.append(\n",
    "            self._convert_obs(state)[:, :, np.newaxis])\n",
    "        self.episode_actions.append(action)\n",
    "        self.episode_rewards.append(reward)\n",
    "\n",
    "    def decide(self, state):\n",
    "        \"\"\" This function feeds the observed state to the network, which returns a distribution\n",
    "            over possible actions. Sample an action from the distribution and return it.\"\"\"\n",
    "\n",
    "        probs = np.ravel(self.model(self._convert_obs(state)\n",
    "                         [np.newaxis, :, :, np.newaxis]))\n",
    "        return np.random.choice(len(probs), p=probs)\n",
    "\n",
    "    def register_reset(self, observations):\n",
    "        action = np.random.randint(5)\n",
    "        return action\n",
    "\n",
    "    def compute_action(self, observations, info):\n",
    "        action = np.random.randint(5)\n",
    "        return action\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\" When this function is called, the accumulated episode observations, actions and discounted rewards\n",
    "            should be fed into the network and used for training. Use the _get_returns function to first turn \n",
    "            the episode rewards into discounted returns. \n",
    "            Apply simple or adaptive baselines if needed, depending on parameters.\"\"\"\n",
    "\n",
    "        states = np.stack(self.episode_observations)\n",
    "        discounted = self._get_returns().reshape((-1, 1))\n",
    "\n",
    "        # compute baseline and train\n",
    "        baseline = self.value_model.predict_on_batch(states)\n",
    "        self.value_model.train_on_batch(states, discounted.reshape((-1, 1)))\n",
    "\n",
    "        # compute discounted rewards and remove baseline\n",
    "        discounted_rewards = discounted - baseline\n",
    "\n",
    "        # train model\n",
    "        self.model.train_on_batch(\n",
    "            states,\n",
    "            tf.keras.utils.to_categorical(self.episode_actions, num_classes=5),\n",
    "            sample_weight=discounted_rewards\n",
    "        )\n",
    "        # reset observations for next episode\n",
    "        self.episode_observations, self.episode_actions, self.episode_rewards = [], [], []\n",
    "\n",
    "    def moving_average(self, discounted):\n",
    "        # save the mean of the rewards for the moving average\n",
    "        self.saved_rewards.append(np.mean(discounted))\n",
    "        # only keep last n averages\n",
    "        if len(self.saved_rewards) > self.n_moving_avg:\n",
    "            self.saved_rewards = self.saved_rewards[1:]\n",
    "        # compute baseline from the moving average\n",
    "        return np.mean(self.saved_rewards)\n",
    "\n",
    "    def _get_returns(self):\n",
    "        \"\"\" This function should process self.episode_rewards and return the discounted episode returns\n",
    "            at each step in the episode. Hint: work backwards.\"\"\"\n",
    "\n",
    "        discounted_rewards = []\n",
    "        cumulative_total_return = 0\n",
    "        # iterate the rewards backwards and and calc the total return\n",
    "        for reward in self.episode_rewards[::-1]:\n",
    "            cumulative_total_return = (\n",
    "                cumulative_total_return*self.gamma)+reward\n",
    "            discounted_rewards.append(cumulative_total_return)\n",
    "        # reverse sequence\n",
    "        discounted_rewards.reverse()\n",
    "        # convert to numpy array\n",
    "        return np.array(discounted_rewards)\n",
    "\n",
    "    def _build_network(self):\n",
    "        \"\"\" This function should build the network that can then be called by decide and train. \n",
    "            The network takes observations as inputs and has a policy distribution as output.\"\"\"\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "                  kernel_initializer='he_uniform', input_shape=(16, 16, 1)))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(20, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(5, activation='softmax'))\n",
    "        model.compile(loss=\"categorical_crossentropy\",\n",
    "                      optimizer=Adam(learning_rate=self.plr))\n",
    "\n",
    "        value_model = Sequential()\n",
    "        value_model.add(Conv2D(32, (3, 3), activation='relu',\n",
    "                        kernel_initializer='he_uniform', input_shape=(16, 16, 1)))\n",
    "        value_model.add(MaxPooling2D((2, 2)))\n",
    "        value_model.add(Flatten())\n",
    "        value_model.add(Dense(20, activation='relu',\n",
    "                        kernel_initializer='he_uniform'))\n",
    "        value_model.add(Dense(1, activation='softmax'))\n",
    "        value_model.compile(loss='mean_squared_error',\n",
    "                            optimizer=Adam(learning_rate=self.vlr))\n",
    "\n",
    "        self.model = model\n",
    "        self.value_model = value_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
